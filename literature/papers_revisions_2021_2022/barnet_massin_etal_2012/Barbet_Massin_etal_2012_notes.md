
## Parts read

- The whole paper

## Notes taken after reading Methods and Results

- Note that the prevalence analyses used True absences, not PAs, so it is not the same than our approach. We can say that our absences are closer to true absences than other PAs, but they are not true absences.

- Weighted means: balancing the weight of presences vs. absences (question c), such that all presence data combined had the sameweight as the total weight ofthe absence data.

- In this paper they used TSS, and we have obtained very high TSS values, close to the one showed by this paper in multiple cases. 
	- A reviewer could say that our TSS is not valid because we do not know the complete disribution of the species. We can have biases of sample.... THIS IS NOT our case. We have complete ranges of pines distributions thanks to the maps of Critfield.

- Figure 4 supports that our ratio PA/occurrences and number of replicates (random partitions of the presence/PA data) is ok:
	- Note that for the replicate analyses, they vary the number of times they repeate the sampling of PAs. For example, you have a random set of presences, you extract a random set of PAs, do the modelling. Take another random set of PAs, repeate the analysis. This would be two random replicates. Then they did it three times, this would be 3 random replicates, and so on. They did for each random set of presences having a total of 20 random sets of presences. They are testing the influence of the number of PA replicates. They use several random sets in general, to control for the variability coming from random selections.
	- In our case, in each replicate, we select different PAs and occurrences.
	- Always considering 100 climatically biased occurrences (the whole environmental range has not been covered).
	- With 100 weighted PAs, there is an increase of TSS until reach around 14 replicates in GLM and GAM. With 10 and 100 times more of weighted PAs, the TSS is very high even with low number of replicates (more than 4). We have used 12 replicates. 
		- Note that the TSS was the highest for 1000 and 10000 weighted PAs in GLM and GAM, suggesting that more PAs than occurrences are needed for these models and weighted is better.
	- In the case of random forest, they did not have weighted PAS, so we cannot compare. We can compare with the classification (MDA and CTA) and the other machine-learning method (BRT).
		- With 1000 weighted PAs (ten times more PAs), TSS did not significantly increase after 2, 3-4 and 2 replicates for MDA, CTA and BRT. 
		- Note that TSS is relatively high for 1000 weighted PAs in MDA and BRT, but in CTA it is clearly lower than for 100 PAs. The pattern for non-weighted PAs is very similar between RF, BTR and MDA, so I would say that maybe weigthed PAs for RF would work more similar to BRT and MDA, suggesting high TSS for 1000 weighted PAs, but I do not have evidence.

- Figure 5 shows differences in TSS across PA approach across different number of occurrences and biases/non biased occurrences.
	- We are going to focus on 100 presences, which would a similar number of PAs for 100 PA (yellow and red) and ten times less for 1000 PAs (green and light blue). 10000 PAs (would be purple and dark blue) would be 100 times less occurrences. We also focused on climatically- and spatially- biased occurrences.
	- In GLM, 10000, 1000 weigthed PAs and 100 weighted and non-weighted PAs showed the highest TSS, but in general, differences are not very high. In GAM, I cannot see the colors for weighted PAs, but I guess they are there because in unbiases there is a little bit of green. In the paper, they say that "GLM and GAM showed little variation in predictive accuracy in response to the number of pseudo-absences, but the predictive accuracy increased when using pseudo-absences with equal weight for presences and absences".
	- For the rest, they say "weight for presences and absences. Secondly, for CTA, BRT and RF, predictive accuracy was highest when approximately the same number of pseudo-absences was used as the number of presences (Fig. 3). For CTA and BRT, when the number of pseudo-absences differed from the number of presences, an equal weight for presences and absences gave better model pre- dictive quality.". The 1000 and 1000W have lower TSS in CTA and MDA, but I do not see it in BRT. Altough RF has no weighted PAs, we can see that the pattern for non-weighted PAs is similar to CTa and MDA, so maybe with weighted we would have the same. However, for BRT, 1000 non-weighted PAs are below, but 1000W is above in TSS. So I cannot say what could have to RF with weighted PAs.

	- IMPORTANT: 
		- In the Discussion, the authors say that the fact for classifications techniques, the accuracy better with lower PAs, is not cause just because we add more false absences, because the same it is seeing when using true absences in the experiment of prevalence (experiment A).
			- This difference in terms of the optimal number of pseudo-absences to use in each replicate for different SDM could not be solely attributed to the poor performance of classification and machine-learning techniques when the number of false absences increases (which is automatically the case when the number of pseudo-absences increases), because the study regarding the influence of prevalence over model accuracy, performed with true absences only, lead to the same conclusions. This difference could therefore be attributed to the intrinsic properties of the different SDM with regard to prevalence.
		- I DO NOT AGREE FOR RANDOM FOREST. 
			- In the case of RF, you can see in figure 3 as TSS reach a plateau at a prevalence 0f -0.1, that is, one tenth of presences respect absences (0.1). This is different from CTA, for which the highest TSS is reach with equal number of presences and absences. 
			- Therefore, in the case of RF, adding more true absences does not have the same impact. With true absences, we can 10 times more absences than presences and have very high TSS.
			- What this means for us? In our study, we are not just using pseudoabsences, but creating random points across different environmental layers in places of KNONW-ABSENCE OF PINES. We know very well the distribution of pines, so our PAs, are closer to true absences than in other studies. This can explain the high TSS for random forest in our study.

- Figure 6 for the way to create PAs:
	- They calculate the average TSS across different numbers of PAS, weighted and unweighted and the random selection of presences. So we cannot differentiate exactly our case.
	- They compared:
		- Random: Random selection from all points within the studied area excluding available presence points.
		- SRE: Random selection of points from all points outside of the suitable area estimated by a rectilinear surface envelope from the presence sample (surface range envelope model using only presence-only data, Thuiller et al. 2009). I guess they use only points in the studied area, but I am not completely sure. 
			- This will probably the most similar to our approach, but note that we also applied an stratified sampling.
		- 1 far: Random selection of any point located at least one degree in latitude or longitude from any presence point.
		- 2 far: Random selection of any available point located at least two degrees away from any presence point.
	- Note that pseudo-absences can be presences that were not retained within the presence sample used to build the models (i.e. false absences).
		- I understand that a PA can occupy a place where an occurrence is present in other replicate. Note the following: "For each combination of parameters, 20 replicates with different presence data selections were performed to account for the variability in model accuracy because of the random sampling of presence data (Fig. 2). For each presence data sample, several replicates with different pseudo-absences selections were per- formed to further account for the variability because of the random sampling of pseudo-absence data". 
		- Therefore, a sample can have some presences and other sample can have other occurrences, so the places where PAs can be created are different.
	- According to the authors, for GLM, GAM and MARS, randomly selected PAs show the highest TSS, but I do not see it very clearly in the plot. SRE shows higher TSS in some cases, or it is very close to random. This is specially true for GLM. 
		- The conclusion for these models would be that you can calculate PAs in the same regions than occurrences are around. But I am still not sure if this is applicable to our GLMs and GAMs.
	- For the other four SDM (MDA, BRT, CTA and RF), there was less variation in the results obtained for each different method used to select pseudo-absences, but pseudo-absences selected with geographical exclusion (‘2?far’) yielded significantly better models with few presences, whereas pseudo-absences selected with climatic exclusion (‘SRE’) yielded better models with more presences.
		- The conclusion would be that you need to separate PAs from the region of occurrences, BUT:
			- Again, they did not use weighted PAs for RF.
			- In addition, any of the methods for creating PAs is exactly the same. I can consider their average to see the influence of other factors, but pick jsut one to be similar to our approach, more difficult....


## Notes after reading Discussion

- It is interesting that the authors considered that big areas of research can lead to include PAs very different from presences and then, not very informative. In that case, it can useful to select more PAs to increase the probability to select PAs closer to the presences, and hence more informative. This is another argument to avoid selecting much PAs far away from knwon-presences. IMPORANT: They did not test this, if you want to use this point to justify the use of a limited buffer around the distribution and not the whole world, you have to read "Selecting pseudo-absence data for presence-only distribution modeling: How far should you stray from what you know?"
	- As well as being influenced by the number of pseudo-absences and the method used to generate them, model performance also relies on the spatial extent of the study. Indeed, model per- formance is lower when pseudo-absences are taken from either a restricted or particularly broad area (Van Der Wal et al. 2009). Pseudo-absences are meant to be compared with the presence data and help differentiate the environmental condi- tions under which a species can occur or not. Therefore, pseudo-absences taken too far from the presence data in the environmental space would not be very informative. As pseudo-absences that are very distant from all presence points (from a geographical point of view) are more likely to exhibit environmental conditions that are very different from those for the presence data, a larger spatial extent of the study will lead to the selection of a higher proportion of less informative pseudo-absences. The optimal number of pseudo-absences to generate in each run is therefore likely to depend on the spatial extent ofthe study, which influences environmental variability. At a given spatial resolution, a higher number of pseudo- absences may be needed to optimise model performance for a larger spatial extent of the study, to ensure the selection of enough informative pseudo-absences.


- False positives vs false negatives
	- I understand that you may want to minimize false positives when doing reservation planning, when you want to prioritize a region for conservation. 
		- False positives consist in absences (negative) that are classified as presences (positive), thus these are false positives. If you minimize these false positives, you would have more confidence that points classified as presences are not absences. Of course, within the points absences you will probably have absences (true negative), but also presences (false negative), but what you really want is to have a region with high confidence about the presence of the species, that is, a point classiffied as presence that is an actual presence (true positive), not a absence (false positive). In that way, you can prioritize this region for conservation. In that case, they saw better results for random PAs, and in general, a high number of PAs (except for GLM and GAM where less number is better). You can see this in figure S4, shosing the influence of the number of PAs and method on specificity, which is the proportion of absences correctly defined as absences (I have not checked with great detail this figure). So true absences are not defined as presences, and we have low number of absences in the set of points defined as presences.
			- From the paper: In contrast, other studies may wish to maximise speci- ficity, so that the predicted distribution of a species would only be the area where the species is highly likely to be pres- ent. This is particularly true for studies on reserve planning (Marini et al. 2009). High specificity ensures that the percent- age of true absences predicted as presences will be minimised. In such cases, the random selection of pseudo-absences will maximise specificity. As for the number of pseudo-absences to generate in each replicate to maximise specificity, it depends on the number of presence points available, but overall a large number of pseudo-absences tends to yield bet- ter specificity for all SDM except GLM and GAM for which fewer pseudo-absences are better.
		- False negatives consist in presences (positive) that are classified as absences (negative), thus these are false negatives. If you minimize these false negatives, you would have more confidence that points classified as absences are not presences. Of course, within the points presences you will probably have presences (true positives), but also absences (false positives), but what you really want is to have a region with high confidence about the absence of the species, that is, a point classiffied as absence that is an actual absence (true negative), not a presence (false negative). In that way, you can avoid this regions for doing new samplings in the search of new areas where the species can be present. In that case, they saw better results for SRE and far points to create PAs, and in general, a low number of PAs (except for GLM and GAM where much number is better). You can see this in figure S3, showing the influence of the number of PAs and method on sensitivity, which is the proportion of presences correctly defined as presences (I have not checked with great detail this figure). So true presences are not defined as absences, and we have low number of presences in the set of points defined as absences.
			- When the modelling goal is to identify potential presences of rare species for new survey efforts (Engler, Guisan & Rechsteiner 2004), high sensitivity is preferred, even if it generates overprediction. High sensitivity ensures that the percentage of true presences predicted as absences will be minimised. In such studies, the ‘SRE’, ‘1 and 2?far’ methods can be used as well as other methods for selecting pseudo- absences outside both spatially and climatically suitable areas (Hengl et al. 2009; Lobo, Jimenez-Valverde & Hortal 2010). The selection of fewer pseudo-absences in each replicate also yielded better sensitivity (except for GLM and GAM, for which large amounts of pseudo-absences with an equal weighting of presences vs. absences still yielded better sensi- tivity).

	- All these results regarding sensitivity and specificity are dependent on the threshold used to produce binary distributions. The use of another commonly used threshold (minimising the difference between sensitivity and specificity) could yield slightly different results as it tends to favour specificity, whereas the threshold we used tends to favour sensitivity (Jimenez-Valverde & Lobo 2007). According to the Methods, Binary transformation was carried out using the threshold that maximised the true skill statistics. So according to this, we would be favoring sensitivity, minimizing false negatives, so we would have more confidence that points classified as absences are not presences, so we could be more sure where the species is not present. I have not check the reference of Lobo.

	- In our case, 
		- We had to use TSS. We have evidence from Barbet-Massin et al 2012 that GLM and GAM have higher accuracy (TSS) with more PAs than presences, in the case of RF we do not know because they did no use the different weight for presences and absences in RF. In addition, for RF, when using true absences, high accuracy is reached with more absences than presences. Therefore, it makes sense to use more presences than absences. Under these scenario, TSS is a metric of model performance less sensitive to disbalances between the number of presences and absences, so again, makes sense to use this metric to select the best threshold to binarize predicitions. 
		- ASSUMING that what they say in the discussion is true (the cited Jimenez-Valverde & Lobo 2007), we prioritized sensitity (having confidence of regions of absence) because we used TSS. In addition it produces higher sensitity to use a high number of PAs for GLM and GAM (again, we do not know for RF because they used a different method of weighting). Also it is better the far and SRE methods, being SRE the most similar to our approach for creaing PAS, altough it is not the same. So we are really doing well for enhancing sensitivity. 
		- I think that it would be worse to focus on specificity. If you minimize false positives, you are focusing in the presences that are more clearly presences, leaving out real presences and the corresponding environmental conditions that are part of the CURRENT AND VISIBLE niche. We would be losing part of the niche, making worse the problem of climate disequilibrium, because the range of climate conditions considered for predict future suitability would be smaller, maybe underestimating even more the response of species to climate change.
		- In contrast, when we focus on sensitivity, we are delimiting regions of really true absence, while in the presences we would have presences and absences. Therefore, we are covering more of the current niche because we are probably capturing most of the current presences. We are also including false positives, absences, but maybe between these absences close to presences there are regions that are actually close to the realized niche and indeed included in the fundamental niche, being useful for climate change. THIS IS JUST AN IDEA, I HAVE NOT EVIDENCE.
		- For the future conditions, we then project suitability and the ensamble were done selectiong only those regions with high ceratinty of suitability across modeling choices. 
			- Here we are focusing in regions that are probably suitable under future conditons. This could makes sense.
			- We have used a broad niche definition not limiting too much suitable conditions to clear presences and adding the phylogenetic correction. All of this can help with the assumption of climate equilibrium, we have maybe extended the realized niche getting closer to the fundamental niche.
			- Once we have modelled with a broad definition of the niche and project to the future, we now can focus on selection the regions with more confidenced of being suitable in the future according to this broad niche. Because at the end we want predict where pines will be present in the future.
            - It makes sense, we are comparing the size of regions with high certainty of suitability between current and future conditions. We did it for range loss and change, and for change in pine richness. We are interested to see what happens with regions that are likely to be suitable, they increase? they decrease? Sure, maybe othe regions are suitable, but if we see a decrease we can say that high-confidence regions for suitability are decreased. If increases, we can say that high confidence regions are increasing!