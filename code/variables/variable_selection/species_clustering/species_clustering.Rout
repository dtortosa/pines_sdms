
R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ###definimos el directorio de trabajo
> setwd("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus")
> 
> #library
> require(foreach) #for repeat a process several times 
Loading required package: foreach
> require(doParallel) #for parallel
Loading required package: doParallel
Loading required package: iterators
Loading required package: parallel
> 
> 
> #######################################
> ###########Load variables##############
> ####################################### 
> list_variables = list.files("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/datos/finals", full.names=TRUE, pattern=".asc")
> require(raster)
Loading required package: raster
Loading required package: sp
> variables = stack(list_variables)
> 
> 
> ##################################
> ########Extract D2################
> ##################################
> 
> #requireed functions
> Dsquared = function(model, adjust = TRUE) {
+   # version 1.1 (13 Aug 2013)
+   # calculates the explained deviance of a GLM
+   # model: a model object of class "glm"
+   # adjust: logical, whether or not to use the adjusted deviance taking into acount the nr of observations and parameters (Weisberg 1980; Guisan & Zimmermann 2000). More data used for fitting enhance d2, whilst more parameters lower it.
+   d2 <- (model$null.deviance - model$deviance) / model$null.deviance
+   if (adjust) {
+     n <- length(model$fitted.values)
+     p <- length(model$coefficients)
+     d2 <- 1 - ((n - 1) / (n - p)) * (1 - d2)
+   }
+   return(d2)
+ } #D2. If you have problem with this you can use Dsquared function of modEvA (http://modeva.r-forge.r-project.org/). 
> 
> ###list ocurrences
> list_species = read.table("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/presences/species.txt", sep="\t", header=T)
> 
> #extract epithet from species list
> epithet_species_list = NULL
> for(i in 1:nrow(list_species)){
+ 
+     #selected species
+     selected_species = as.vector(list_species[i,])
+ 
+     #extract epithet
+     epithet_species_list = append(epithet_species_list, strsplit(selected_species, split=" ")[[1]][2])
+ }
> summary(is.na(epithet_species_list)) #all false
   Mode   FALSE 
logical     113 
> 
> #make a function
> d2_total = function(i){
+     print(i)
+     ocurrences_data = read.csv(paste("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/results/pseudo_absences", paste(i, "complete.presences.csv", sep="_"), sep="/"), header=TRUE)
+ 
+     #extract values of variables in each point for creating predictors variables
+     predictors = as.data.frame(extract(x=variables, y=ocurrences_data[,c("longitude", "latitude")]))
+ 
+     #create a unique data frame with response (points) and predictors (environmental variables)
+     data = cbind(ocurrences_data, predictors)
+ 
+     #loop for extract D2 for each variable 
+     d2=NULL
+     for (k in 1:ncol(predictors)){
+         model = glm(data$presence ~ poly(data[,c(5:30)][,k], 2), family=quasibinomial(link=logit), weights=data$precision_weight) #I have used quasibinomial becasue I have weighted the response variable with precision_weight variable, thus we have decimals between 0 and 1. With binomial family we would obtain the following error "fitted probabilities numerically 0 or 1 occurred", but the results are the same.
+         d2 = append(d2, Dsquared(model))
+     }
+ 
+     d2_data_frame = data.frame()
+     d2_data_frame = rbind(d2_data_frame, d2)
+     d2_data_frame = cbind(i, d2_data_frame)
+     names(d2_data_frame) = c("species", names(variables))
+     d2_data_frame
+ }
> 
> #create a vector with all species
> species = epithet_species_list
> 
> # set up cluster
> clust <- makeCluster(4) 
> registerDoParallel(clust)
> 
> # run for all species
> d2.final = foreach(i = species, .packages = "raster", .combine="rbind") %dopar% { 
+     d2_total(i = i)
+ } #the "stringr" package is used for "str_split_fixed" function
> 
> #stop the cluster 
> stopCluster(clust)
> 
> #check if all is correct
> str(d2.final)
'data.frame':	113 obs. of  27 variables:
 $ species: Factor w/ 113 levels "albicaulis","aristata",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ bio1   : num  0.507 0.384 0.71 0.379 0.429 ...
 $ bio10  : num  0.354 0.132 0.666 0.168 0.222 ...
 $ bio11  : num  0.558 0.474 0.758 0.508 0.471 ...
 $ bio12  : num  0.0537 0.1804 0.7729 0.4964 0.2765 ...
 $ bio13  : num  0.102 0.184 0.864 0.172 0.124 ...
 $ bio14  : num  0.0407 0.1908 0.8751 0.3757 0.4225 ...
 $ bio15  : num  0.128 0.295 0.79 0.137 0.025 ...
 $ bio16  : num  0.114 0.19 0.864 0.205 0.128 ...
 $ bio17  : num  0.0481 0.1866 0.8811 0.4285 0.4263 ...
 $ bio18  : num  0.034 0.221 0.692 0.42 0.358 ...
 $ bio19  : num  0.103 0.179 0.933 0.505 0.234 ...
 $ bio2   : num  0.0935 0.5297 0.4915 0.4624 0.0506 ...
 $ bio3   : num  0.439 0.602 0.724 0.592 0.3 ...
 $ bio4   : num  0.409 0.514 0.793 0.603 0.358 ...
 $ bio5   : num  0.2509 0.0943 0.6244 0.1891 0.2866 ...
 $ bio6   : num  0.497 0.481 0.66 0.52 0.368 ...
 $ bio7   : num  0.257 0.421 0.508 0.577 0.283 ...
 $ bio8   : num  0.292 0.238 0.517 0.221 0.136 ...
 $ bio9   : num  0.524 0.425 0.416 0.356 0.389 ...
 $ carbon : num  0.114 0.279 0.361 0.321 0.176 ...
 $ cec    : num  0.199 0.242 0.196 0.303 0.134 ...
 $ clay   : num  0.188 0.234 0.698 0.643 0.096 ...
 $ depth  : num  0.00608 0.0263 0.67139 0.25163 0.20838 ...
 $ ph     : num  0.00976 0.15251 0.58642 0.25326 0.14831 ...
 $ sand   : num  0.0558 0.1055 0.6451 0.549 0.1814 ...
 $ silt   : num  0.14 0.103 0.438 0.313 0.26 ...
> 
> #write the rank
> write.csv(d2.final, "/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/clustering_results/d2_species_variables.csv", row.names=FALSE)
> 
> #create the rank 
> d2_rank  = data.frame(t(apply(-d2.final[-1], 1, rank, ties.method='min')))
> d2_rank = cbind(d2.final$species, d2_rank)
> names(d2_rank)[which(names(d2_rank)=="d2.final$species")] <- "species" 
> write.csv(d2_rank, "/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/clustering_results/d2_rank_species_variables.csv", row.names=FALSE)
> 
> #############################
> ######Cluster analysys#######
> #############################
> 
> #load the data set with the rank
> d2_rank = read.csv("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/clustering_results/d2_rank_species_variables.csv", header=TRUE, row.names=NULL)
> str(d2_rank)
'data.frame':	113 obs. of  27 variables:
 $ species: Factor w/ 113 levels "albicaulis","amamiana",..: 1 3 2 4 5 6 7 8 9 10 ...
 $ bio1   : int  3 8 11 13 2 3 16 7 5 17 ...
 $ bio10  : int  7 22 15 25 15 11 26 18 8 20 ...
 $ bio11  : int  1 5 9 7 1 2 9 3 2 14 ...
 $ bio12  : int  21 19 8 9 12 24 24 23 11 9 ...
 $ bio13  : int  18 18 4 24 23 16 21 16 16 18 ...
 $ bio14  : int  23 15 3 14 4 5 17 6 15 4 ...
 $ bio15  : int  14 9 7 26 26 7 15 2 18 26 ...
 $ bio16  : int  15 16 5 22 22 17 22 17 14 16 ...
 $ bio17  : int  22 17 2 11 3 9 19 8 13 3 ...
 $ bio18  : int  24 14 13 12 8 22 20 24 10 8 ...
 $ bio19  : int  17 20 1 8 14 21 13 19 17 7 ...
 $ bio2   : int  19 2 22 10 25 14 23 11 12 23 ...
 $ bio3   : int  5 1 10 3 9 1 5 1 1 1 ...
 $ bio4   : int  6 3 6 2 7 4 4 5 7 2 ...
 $ bio5   : int  10 25 18 23 10 15 25 22 4 10 ...
 $ bio6   : int  4 4 16 6 6 6 8 4 3 12 ...
 $ bio7   : int  9 7 21 4 11 10 6 9 9 6 ...
 $ bio8   : int  8 12 20 21 20 12 18 13 20 22 ...
 $ bio9   : int  2 6 24 15 5 8 14 10 6 19 ...
 $ carbon : int  16 10 25 16 18 13 12 12 19 13 ...
 $ cec    : int  11 11 26 18 21 18 11 15 22 24 ...
 $ clay   : int  12 13 12 1 24 19 1 14 23 25 ...
 $ depth  : int  26 26 14 20 16 25 2 26 24 11 ...
 $ ph     : int  25 21 19 19 19 23 10 25 21 15 ...
 $ sand   : int  20 23 17 5 17 26 3 20 25 21 ...
 $ silt   : int  13 24 23 17 13 20 7 21 26 5 ...
> 
> ###list ocurrences
> list_species = read.table("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/presences/species.txt", sep="\t", header=T)
> 
> #extract epithet from species list
> epithet_species_list = NULL
> for(i in 1:nrow(list_species)){
+ 
+     #selected species
+     selected_species = as.vector(list_species[i,])
+ 
+     #extract epithet
+     epithet_species_list = append(epithet_species_list, strsplit(selected_species, split=" ")[[1]][2])
+ }
> summary(is.na(epithet_species_list)) #all false
   Mode   FALSE 
logical     113 
> 
> #Load the grid where all the species of the group are stored.
> require(raster)
> distribution = NULL
> for (i in epithet_species_list){
+     distri = raster(paste("/Volumes/GoogleDrive/My Drive/science/phd/dispersal_heterogeneity/datos/DATOS/MAPS/p", paste(i, "01.img", sep="_"), sep="_"))
+     distribution = append(distribution, distri)
+ }
> 
> all_species_grid = stack(distribution)
> nlayers(all_species_grid) #113, on for each species
[1] 113
> names(all_species_grid) <- epithet_species_list #change the name for match with name sof rank data set speices
> 
> #calculate euclidean distance between species 
> dendogram_table = d2_rank[-1] #select all columns except species column
> rownames(dendogram_table) = epithet_species_list #put species names as row names
> species.dist<-dist(abs(dendogram_table), method="euclidean") #We will use the typical euclidean distance: The sum of the squared differences for each value of two vectors ((x[1]-y[1])+(x[2]-y[2])....). Usual distance between the two vectors (2 norm aka L_2), sqrt(sum((x_i - y_i)^2)).
> 
> #make a dendrongram with hclust
> species_dendogram<-hclust(species.dist) 
> plot(species_dendogram)
> 
> #save it
> png("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/clustering_results/species_dendogram.png", width=1500, height=800, pointsize=30 )
> plot(species_dendogram, cex=0.5)
> dev.off()
pdf 
  2 
> 
> #load the package neccesary
> require(NbClust)
Loading required package: NbClust
> source("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/nbclust_modified.R") #load the nbclust function modified. The rationale is that I obtain an error with complete method: "Error in solve.default(W) : system is computationally singular: reciprocal condition number = 1.12144e-16". 
> #solve(w) is used internally by nbclust to calculate the index of friedman, one of the index for evaluate the quality of the number of clusters. Thus is not ver very important because we have other index. The problem is caused because solve is a numerical way to calculate the inverse of the covariance matrix. Unfortunately, if some of the numbers used in the inverse calculation are very small, it assumes that they are zero, leading to the assumption that it is a singular matrix. This is why it specifies that they are computationally singular, because the matrix might not be singular given a different tolerance. The solution is use a smaller tolerance in solve functuin, like solve(..., tol = 1e-19). This should be fine since you get reciprocal condition number = 1.16873e-16 (see http://stackoverflow.com/questions/21451664/system-is-computationally-singular-error). 
> 
> 
> #create a function for make clusters and plot the results
> species_clustering = function(min_groups, max_groups, method){
+ 
+     #load the data set with the rank
+     d2_rank = read.csv("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/clustering_results/d2_rank_species_variables.csv", header=TRUE, row.names=NULL)
+     str(d2_rank)
+ 
+     ###list ocurrences
+     list_species = read.table("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/presences/species.txt", sep="\t", header=T)
+ 
+     #extract epithet from species list
+     epithet_species_list = NULL
+     for(i in 1:nrow(list_species)){
+ 
+         #selected species
+         selected_species = as.vector(list_species[i,])
+ 
+         #extract epithet
+         epithet_species_list = append(epithet_species_list, strsplit(selected_species, split=" ")[[1]][2])
+     }
+     summary(is.na(epithet_species_list)) #all false
+ 
+     #Load the grid where all the species of the group are stored.
+     require(raster)
+     distribution = NULL
+     for (i in epithet_species_list){
+         distri = raster(paste("/Volumes/GoogleDrive/My Drive/science/phd/dispersal_heterogeneity/datos/DATOS/MAPS/p", paste(i, "01.img", sep="_"), sep="_"))
+         distribution = append(distribution, distri)
+     }
+ 
+     all_species_grid = stack(distribution)
+     nlayers(all_species_grid) #113, on for each species
+     names(all_species_grid) <- epithet_species_list #change the name for match with name sof rank data set speices
+ 
+     # Create the empty results list
+     final_results <- list()
+ 
+     #run the cluster
+     cluster = NbClust(data=d2_rank[-1], #data: matrix or dataset.
+         diss=NULL, #dissimilarity matrix to be used (matriz de distancias). You can used one make it before, or if NULL the matrix will be created by means of the distance methos indicated in the next argument. I have test if there is differences between make the matrix before or use the matrix created by nbclust with kmeans method, and there is not differences (see kmeans_2_10_g_3_distances_nbclust.png). 
+         distance = "euclidean", #distance: the distance measure to be used to compute the dissimilarity matrix (euclidean, maximum, etc..). It is not neccesary if we introduce the matrix created before. 
+         min.nc=min_groups, #minimal number of clusters, between 1 and (number of objects - 1)
+         max.nc=max_groups, #maximal number of clusters, between 2 and (number of objects - 1), greater or equal to min.nc. By default, max.nc=15.
+         method = method, #the cluster analysis method to be used. This should be one of: "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid", "kmeans". Achilleas told me thay I should use complete (The distance between two clusters and is the maximum distance between two points x and y). Info of kmeans (see documentation of NbClust function and diapo7 in http://slideplayer.com/slide/5178523/).
+         index = "all") #the index to be calculated. They inform about the quality of each proposed clustering (2,3,4,... clusters). If you use more than 1 index, the function summarize how many index consider each number of cluster as optimal, thus how much more index use better). index="all" include a lot of index, but not all. 
+ 
+     #Extracting clustering parameters
+     cluster$All.index #results of all index for each number of clusters. Values of indices for each partition of the dataset obtained with a number of clusters between min.nc and max.nc.
+     cluster$Best.nc #Best number of clusters proposed by each index and the corresponding index value.
+     cluster$All.CriticalValues #Critical values of some indices for each partition obtained with a number of clusters between min.nc and max.nc.
+ 
+     #Extracting clusterings GROUP details
+     groups <- cluster$Best.partition #Partition that corresponds to the best number of clusters
+     print(table(groups)) #number of species by group
+     num_groups <- length(unique(groups)) #numbe of groups
+     group_ids <- unique(groups) #id of the groups
+     final_results[[1]] <- cluster #save all results of cluster in final_results
+     d2_rank = cbind(d2_rank, groups) #combine d2_rank with the group factor
+ 
+     #export the group variable with the species names
+     write.csv(d2_rank, paste("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/_tables/", method, "_", min_groups ,"_", max_groups,"_","g","_", num_groups,".csv",sep=""), row.names = FALSE)
+ 
+     # Creating the spatial distribution of groups
+     groups_final <- list() 
+     species_grids <- list()
+     for(i in 1:length(group_ids)){ #for each group
+         group_grids <- list()
+         groups_final[[i]] <- d2_rank[which(groups == group_ids[i]),] #select rows of d2_rank of the group [i]
+         sp_names <- as.character(d2_rank[which(groups == group_ids[i]),]$species) #select the speceis of the group [i]
+         species_grids[[i]] <- all_species_grid[[match(sp_names,names(all_species_grid))]] #select the distribution grids of the species of the group [i]
+     }
+     final_results[[2]] <- species_grids #species_grids is a list with the distribution rasters of each species
+ 
+     # Exporting the rasters with the spatial distribution of groups
+     for(j in 1:num_groups){
+         print("Exporting raster")
+         print(j)
+         print("**************")
+         writeRaster(species_grids[[j]], paste("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/_grids/", "_", min_groups ,"_", max_groups,"_","g","_", num_groups,"_group_",j ,"_species_",nlayers(species_grids[[j]]), method,".grd",sep=""), overwrite=T)
+     } #for each group plot all distribution rasters
+ 
+     # Plotting the spatial distribution of groups
+     png(paste("/Volumes/GoogleDrive/My Drive/science/phd/nicho_pinus/code/variables/variable_selection/species_clustering/figures/", method, "_", min_groups ,"_", max_groups,"_","g","_", num_groups, ".png",sep=""),width= 3000, height = 2000, bg="white",res=150)
+     par(mfcol=c(2,ceiling(num_groups/2)))
+     for(j in 1:num_groups){ #for each group
+         print("**************")
+         print(j)
+         print("**************")
+         if(length(species_grids[[j]])!=1){ #if there more than 1 layer
+           #writeRaster(species_grids[[j]],paste("S:/psomas/requests/scenario_comparison/species_clustering/_grids/",output_name, "_", min_groups ,"_", max_groups,"_","g","_",num_groups,"_group_",j,"_species_",nlayers(species_grids[[j]]),".grd",sep=""))
+           plot(sum(species_grids[[j]]>0),main=paste("Species:",nlayers(species_grids[[j]]),sep=" ")) #sum all raster of each group, but only the areas with value higher than 1. Plot it. 
+         }
+         if(length(species_grids[[j]])==1){ #if there is only one layer
+           #writeRaster(species_grids[[j]],paste("S:/psomas/requests/scenario_comparison/species_clustering/_grids/",output_name, "_", min_groups ,"_", max_groups,"_","g","_",num_groups,"_group_",j,"_species_",nlayers(species_grids[[j]]),".grd",sep=""))
+           plot(species_grids[[j]]>0,main=paste("Species:",nlayers(species_grids[[j]]),sep=" "))
+         }
+     }
+     dev.off()
+ }
> 
> ###Paralellize
> # set up cluster
> clust <- makeCluster(4) #we use fork, because we don't want to copy for each time the rank and the raster of the species. 
> registerDoParallel(clust)
> 
> #methods to make the clustering
> methods = c("average", "centroid", "complete", "kmeans", "mcquitty", "median", "single", "ward.D", "ward.D2")
> 
> #run for all species
> foreach(method = methods) %dopar% { 
+     species_clustering(min_groups=2, max_groups=10, method=method)
+ } #the "stringr" package is used for "str_split_fixed" function
[[1]]
pdf 
  2 

[[2]]
pdf 
  2 

[[3]]
pdf 
  2 

[[4]]
pdf 
  2 

[[5]]
pdf 
  2 

[[6]]
pdf 
  2 

[[7]]
pdf 
  2 

[[8]]
pdf 
  2 

[[9]]
pdf 
  2 

> 
> #stop the cluster 
> stopCluster(clust)
> 
> proc.time()
    user   system  elapsed 
   6.523    0.818 5167.558 
