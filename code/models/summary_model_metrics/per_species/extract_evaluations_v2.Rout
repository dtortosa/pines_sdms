
R version 3.4.4 (2018-03-15) -- "Someone to Lean On"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

Loading required package: colorout
[Previously saved workspace restored]

> #set working directory
> setwd("/Users/dsalazar/nicho_pinus/results/final_analyses/")
> 
> #required packages
> require(dismo)
Loading required package: dismo
Loading required package: raster
Loading required package: sp
> require(raster)
> 
> #load species names
> list_species = read.table("/Users/dsalazar/nicho_pinus/data/list_species.txt", sep="\t", header=TRUE)
> 
> #extract epithet from species list
> epithet_species_list = NULL
> for(i in 1:nrow(list_species)){
+ 
+     #selected species
+     selected_species = as.vector(list_species[i,])
+ 
+     #extract epithet
+     epithet_species_list = append(epithet_species_list, strsplit(selected_species, split=" ")[[1]][2])
+ }
> #no NAs
> summary(is.na(epithet_species_list)) #all false
   Mode   FALSE 
logical     112 
> #check that all is correct
> paste("Pinus ", epithet_species_list, " ", sep="") == as.vector(list_species$genus.specific_epithet)
  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [25]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[109]  TRUE  TRUE  TRUE  TRUE
> #all TRUE except on, check it
> as.vector(list_species[which(!paste("Pinus ", epithet_species_list, " ", sep="") == as.vector(list_species$genus.specific_epithet)),1])
[1] "Pinus devoniana"
> epithet_species_list[which(!paste("Pinus ", epithet_species_list, " ", sep="") == as.vector(list_species$genus.specific_epithet))] #devoniana is "Pinus devoniana" without space at the end, because of this paste("Pinus ", epithet_species_list, " ", sep="") does not match, but is the same epithet
[1] "devoniana"
> 
> #empty data frame to save results 
> medians_evaluations = as.data.frame(matrix(NA, nrow=length(epithet_species_list), ncol=19))
> colnames(medians_evaluations) <- c("species", "glm_kappa_median", "glm_kappa_sd", "glm_tss_median", "glm_tss_sd", "glm_auc_median", "glm_auc_sd", "gam_kappa_median", "gam_kappa_sd", "gam_tss_median", "gam_tss_sd", "gam_auc_median", "gam_auc_sd", "rf_kappa_median", "rf_kappa_sd", "rf_tss_median", "rf_tss_sd", "rf_auc_median", "rf_auc_sd")
> str(medians_evaluations)
'data.frame':	112 obs. of  19 variables:
 $ species         : logi  NA NA NA NA NA NA ...
 $ glm_kappa_median: logi  NA NA NA NA NA NA ...
 $ glm_kappa_sd    : logi  NA NA NA NA NA NA ...
 $ glm_tss_median  : logi  NA NA NA NA NA NA ...
 $ glm_tss_sd      : logi  NA NA NA NA NA NA ...
 $ glm_auc_median  : logi  NA NA NA NA NA NA ...
 $ glm_auc_sd      : logi  NA NA NA NA NA NA ...
 $ gam_kappa_median: logi  NA NA NA NA NA NA ...
 $ gam_kappa_sd    : logi  NA NA NA NA NA NA ...
 $ gam_tss_median  : logi  NA NA NA NA NA NA ...
 $ gam_tss_sd      : logi  NA NA NA NA NA NA ...
 $ gam_auc_median  : logi  NA NA NA NA NA NA ...
 $ gam_auc_sd      : logi  NA NA NA NA NA NA ...
 $ rf_kappa_median : logi  NA NA NA NA NA NA ...
 $ rf_kappa_sd     : logi  NA NA NA NA NA NA ...
 $ rf_tss_median   : logi  NA NA NA NA NA NA ...
 $ rf_tss_sd       : logi  NA NA NA NA NA NA ...
 $ rf_auc_median   : logi  NA NA NA NA NA NA ...
 $ rf_auc_sd       : logi  NA NA NA NA NA NA ...
> 
> #loop for obtaining medians of evaluations across species
> for(i in 1:length(epithet_species_list)){
+ 
+     #select the [i] species
+     selected_species = epithet_species_list[i]
+     
+     #save the name of the [i] species
+     medians_evaluations[i, "species"] <- selected_species
+ 
+     #unzip data of threshold
+     unzip(paste("threshold/threshold_", selected_species, ".zip", sep=""), exdir="threshold")
+ 
+     #unzip data of AUC
+     unzip(paste("evaluations/evaluations_", selected_species, ".zip", sep=""), exdir="evaluations")
+ 
+     ####################################################
+     ###### ALL evaluation metrics for GLM and GAM ######
+     ####################################################
+     models_to_extract = c("glm", "gam")
+     for(m in 1:length(models_to_extract)){
+ 
+         #selected the [m] model
+         selected_model = models_to_extract[m]
+ 
+         #### TSS and Kappa ###
+         #load threshold data for the [m] model
+         load(paste("threshold/", selected_species, "_", selected_model, "_threshold.rda", sep=""))
+ 
+         #put into an object kappa and tss data
+         kappa_tss_data = eval(parse(text=paste(selected_model, "_threshold", sep="")))
+ 
+         #extract values of kappa and TSS for all data partitions
+         tss_values = sapply(kappa_tss_data, '$.data.frame', "TSS")[1,]       
+         kappa_values = sapply(kappa_tss_data, '$.data.frame', "kappa")[1,]       
+ 
+         #save the median and sd of kappa  across partitions of the data
+         medians_evaluations[i, paste(selected_model, "_kappa_median", sep="")] <- median(kappa_values)
+         medians_evaluations[i, paste(selected_model, "_kappa_sd", sep="")] <- sd(kappa_values)
+             
+         #save the median and sd of TSS   across partitions of the data              
+         medians_evaluations[i, paste(selected_model, "_tss_median", sep="")] <- median(tss_values)
+         medians_evaluations[i, paste(selected_model, "_tss_sd", sep="")] <- sd(tss_values)
+ 
+         #### AUC ####
+         #load AUC data for the [m] model 
+         load(paste("evaluations/", selected_species, "_", selected_model, "_evaluation.rda", sep=""))
+ 
+         #put into an object AUC data of the [m] model
+         evaluation_model = eval(parse(text=paste(selected_model, "_evaluation", sep="")))
+ 
+         #extract AUC data for each data partition
+         auc_values = sapply(evaluation_model, "slot", "auc")
+ 
+         #save the median and sd of AUC for [m] model across partitions of the data
+         medians_evaluations[i, paste(selected_model, "_auc_median", sep="")] <- median(auc_values)
+         medians_evaluations[i, paste(selected_model, "_auc_sd", sep="")] <- sd(auc_values)     
+     }
+ 
+     ###########################################
+     ###### ALL evaluation metrics for RF ######
+     ###########################################
+     
+     ### AUC ###
+     #load the data of AUC for RF
+     load(paste("evaluations/", selected_species, "_rf_evaluation.rda", sep=""))
+ 
+     #save into an object AUC data of RF
+     evaluation_rf = eval(parse(text=paste("rf_evaluation", sep="")))
+ 
+     #extract values of AUC across data partitions
+     auc_values = sapply(evaluation_rf, "slot", "auc")
+ 
+     #save median and sd of AUC across partitions of the data
+     medians_evaluations[i, paste("rf_auc_median", sep="")] <- median(auc_values)
+     medians_evaluations[i, paste("rf_auc_sd", sep="")] <- sd(auc_values)   
+ 
+     #### TSS and Kappa####
+     #We calculate the TSS metric for Random forest. This metric (true skill statistic) can be used for calculating the optimal threshold when your are binarizing continuos probability of presence. Therefore you calculate the optimal value of probability from which you establish a cell as suitable, i.e. the value that best separate absences from presences. However, this value can be also calculated from a confusion matrix obtained from binary probabilities. Therefore, we can calcualte TSS from evaluation data using random forest!!  
+ 
+     #TSS = sensitivity + specificity – 1. Sensitivity = nº true presences / (nº true presences + nº false absences); What proportion of total presences are well established, because of this the denominator is the number of true presences more the number of false absences, i.e. presences bad defined as absences. specificity = nº true absences / (nº true absences + nº false presences); What proportion of total absences are well established, because of this the denominator is the number of true absences more the number of false presences, i.e. absences bad defined as presences. See "Assessing the accuracy of species distribution models: prevalence, kappa and the true skill statistic (TSS)" for details about TSS and kapppa calculations
+ 
+     #The same goes for Kappa, this metric is used to calculate the best threshold, but you only need a confusion matrix to calculate it, so we can obtained from binary probabilities. The formula is very long, you can see it at "Assessing the accuracy of species distribution models: prevalence, kappa and the true skill statistic (TSS)"
+ 
+     #for each partition of the data we extact confusion matrix stored in evaluate objects of dismo
+     tss_values = NULL
+     kappa_values = NULL  
+     for(j in 1:length(evaluation_rf)){
+ 
+         #You have to bear in mind that the evaluate function of dismo takes your presences and absences for evaluation, along with your model and the evaluate, but consider that your probabilities are continuous. Because of this, you have 4-5 confusion matrix, each one calculated with a different threshold, a different probability form which you consider suitable. In the case of our RF, which is a clasification tree, probabilities are 0 and 1, because of this, the first confusion matrix has zero false negative and zero true negatives, this matrix is calculated with a threshold = -0.0001, therefore all points are considered as presences, because all have at least a probabiity of zero. The result is that you select well all the true presences, but of course the rest of points are false presences (absences bad defined), as you don't have any absence. The same goes for the last correlation matrix but in the opposite sense. The threshold used for that confusion maitrx was 1.0001, therefore, all point are considered as absences (no point has a probaility of presence higher than 1). In that way you get all the true absences, but the rest of points as false absences (presences bad defined). Intermediate thesholds, i.e. any threshold that differentiates points with 0 and 1 of probability will have the higher rate of success, because our prediciton maps of RF as binary!! Because of this, we will select the first of the threshold that have the maximum true positive rate + true negative rate. All the confuion matrix with the highest TNR+TPR are ok. 
+ 
+         #sum TPR and TNR for each threshold evaluation_rf[[j]]@TPR and evaluation_rf[[j]]@TNR are vectors with the values of TPR and TNR for all dfferent confusion matrix calculated with the different thresholds
+         tpr_tnr_different_thresholds = evaluation_rf[[j]]@TPR + evaluation_rf[[j]]@TNR
+ 
+         #which are those with the highest sum? select the first one
+         selected_row = which(tpr_tnr_different_thresholds == max(tpr_tnr_different_thresholds))[1]
+ 
+         #from the confusion matrix selected (selected_row; confusin matrix are presented in rows, each in a row), select the different elements: true positive (true presence), false positive (false presence), false negative (false absence) and true negative (true presence)
+         tp = as.vector(evaluation_rf[[j]]@confusion[selected_row, "tp"])
+         fp = as.vector(evaluation_rf[[j]]@confusion[selected_row, "fp"])
+         fn = as.vector(evaluation_rf[[j]]@confusion[selected_row, "fn"])
+         tn = as.vector(evaluation_rf[[j]]@confusion[selected_row, "tn"])
+ 
+         #calculate the total number of presences and absences (this will be used for kappa calculation)
+         n = tp + fp + fn + tn
+ 
+         #calculate sensitivity
+         sensitivity = tp / (tp + fn)#true presences / the sum of true presences more false negtive, i.e. presences bad defined as absences
+ 
+         #calculate specificity
+         specificity = tn / (tn + fp)#true absences / the sum of true absences more false presences, i.e. absences bad defined as presences
+ 
+         #calculate TSS
+         TSS = sensitivity + specificity - 1
+ 
+         #other form more direct to calculate tss
+         TSS2 = ( (tp*tn) - (fp*fn) ) / ( (tp+fn) * (fp+tn))
+ 
+         #check both are equal
+         check_tss = round(TSS - TSS2, 4)
+ 
+         #calculate kappa
+         kappa_1 = ( ( (tp + tn) / n ) - ( ( (tp+fp) * (tp+fn) + (fn+tn) * (tn+fp) ) / (n^2) ) ) / (1 - ( ( (tp+fp) * (tp+fn) + (fn+tn) * (tn+fp) ) / (n^2) ) )
+ 
+         #check that the kappa calculate by me is the same than the kappa obtained with the "evaluate" function
+         check_kappa = round(kappa_1 - evaluation_rf[[j]]@kappa[selected_row], 4)#we use the kappa value obtained for one of the best thresholds, which is indicated with selected_row (see above)
+ 
+         #save it in case all is ok
+         tss_values = append(tss_values, ifelse(check_tss==0, TSS, NA))
+         kappa_values = append(kappa_values, ifelse(check_kappa==0, kappa_1, NA))
+     }
+ 
+     #save the median and sd of TSS and kappa values across partitions
+     medians_evaluations[i, paste("rf_tss_median", sep="")] <- median(tss_values)
+     medians_evaluations[i, paste("rf_tss_sd", sep="")] <- sd(tss_values)      
+     medians_evaluations[i, paste("rf_kappa_median", sep="")] <- median(kappa_values)
+     medians_evaluations[i, paste("rf_kappa_sd", sep="")] <- sd(kappa_values) 
+ 
+     #remove unzipped files
+     file.remove(list.files("threshold", pattern=".rda", full.names = TRUE))
+     file.remove(list.files("evaluations", pattern=".rda", full.names = TRUE))
+ }
> 
> #save the results
> write.table(medians_evaluations, "medians_evaluations/medians_evaluations_v2.csv", sep=",", col.names=TRUE, row.names=FALSE)
> 
> proc.time()
   user  system elapsed 
  3.794   1.126   5.227 
